{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azureml-contrib-reinforcementlearning in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: azureml-train-core~=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-contrib-reinforcementlearning) (1.6.0.post1)\n",
      "Requirement already satisfied: msrest>=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-contrib-reinforcementlearning) (0.6.14)\n",
      "Requirement already satisfied: azureml-core~=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-contrib-reinforcementlearning) (1.6.0.post1)\n",
      "Requirement already satisfied: portalocker>=1.5.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-contrib-reinforcementlearning) (1.7.0)\n",
      "Requirement already satisfied: requests>=2.19.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-contrib-reinforcementlearning) (2.23.0)\n",
      "Requirement already satisfied: msrestazure>=0.4.33 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-contrib-reinforcementlearning) (0.6.3)\n",
      "Requirement already satisfied: azureml-train-restclients-hyperdrive~=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.6.0)\n",
      "Requirement already satisfied: flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (3.7.9)\n",
      "Requirement already satisfied: azureml-telemetry~=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-contrib-reinforcementlearning) (2020.4.5.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-contrib-reinforcementlearning) (1.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-contrib-reinforcementlearning) (0.6.0)\n",
      "Requirement already satisfied: SecretStorage in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (3.1.2)\n",
      "Requirement already satisfied: adal>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.2.3)\n",
      "Requirement already satisfied: pathspec in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.8.0)\n",
      "Requirement already satisfied: azure-mgmt-authorization>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.60.0)\n",
      "Requirement already satisfied: azure-mgmt-storage>=1.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (10.0.0)\n",
      "Requirement already satisfied: contextlib2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.6.0.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.8.1)\n",
      "Requirement already satisfied: azure-mgmt-resource>=1.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (9.0.0)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.8.0)\n",
      "Requirement already satisfied: azure-mgmt-network~=10.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (10.2.0)\n",
      "Requirement already satisfied: pyopenssl in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (19.1.0)\n",
      "Requirement already satisfied: ruamel.yaml>0.16.7 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.16.10)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.9.2)\n",
      "Requirement already satisfied: PyJWT in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.7.1)\n",
      "Requirement already satisfied: jsonpickle in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.4.1)\n",
      "Requirement already satisfied: azure-common>=1.1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.1.25)\n",
      "Requirement already satisfied: jmespath in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.10.0)\n",
      "Requirement already satisfied: urllib3>=1.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.25.9)\n",
      "Requirement already satisfied: azure-mgmt-keyvault>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.2.0)\n",
      "Requirement already satisfied: azure-graphrbac>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.61.1)\n",
      "Requirement already satisfied: pytz in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (2019.3)\n",
      "Requirement already satisfied: docker in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (4.2.0)\n",
      "Requirement already satisfied: backports.tempfile in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.0)\n",
      "Requirement already satisfied: ndg-httpsclient in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.19.1->azureml-contrib-reinforcementlearning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.19.1->azureml-contrib-reinforcementlearning) (2.9)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"->azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.6.1)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"->azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.5.0)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"->azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.1.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"->azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.3)\n",
      "Requirement already satisfied: applicationinsights in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-telemetry~=1.6.0->azureml-train-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.11.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-contrib-reinforcementlearning) (3.1.0)\n",
      "Requirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from isodate>=0.6.0->msrest>=0.5.1->azureml-contrib-reinforcementlearning) (1.12.0)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from SecretStorage->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.4.3)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ruamel.yaml>0.16.7->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.2.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jsonpickle->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.6.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.57.0)\n",
      "Requirement already satisfied: backports.weakref in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from backports.tempfile->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (1.0.post1)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ndg-httpsclient->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata->jsonpickle->azureml-core~=1.6.0->azureml-contrib-reinforcementlearning) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azureml-contrib-reinforcementlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.runconfig import EnvironmentDefinition\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# Azure ML Reinforcement Learning imports\n",
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "from azureml.contrib.train.rl import WorkerConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '1aefdc5e-3a7c-4d71-a9f9-f5d3b03be19a'\n",
    "resource_group = 'EDATRG'\n",
    "workspace_name = 'fepEDATest'\n",
    "\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_compute_target = ws.compute_targets['head-gpu']\n",
    "\n",
    "worker_compute_target = ws.compute_targets['worker-cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pip packages we will use for both head and worker\n",
    "pip_packages=[\"ray[rllib]==0.8.5\"] # Latest version of Ray has fixes for isses related to object transfers\n",
    "\n",
    "# Specify the Ray worker configuration\n",
    "worker_conf = WorkerConfiguration(\n",
    "    \n",
    "    # Azure ML compute cluster to run Ray workers\n",
    "    compute_target=worker_compute_target, \n",
    "    \n",
    "    # Number of worker nodes\n",
    "    node_count=2,\n",
    "    \n",
    "    # GPU\n",
    "    use_gpu=False, \n",
    "    \n",
    "    # PIP packages to use\n",
    "    pip_packages=pip_packages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    \"--iterations\": 200,\n",
    "    \"--numworkers\": 6,\n",
    "    \"--basepolicy\": \"LORHeuristic\"\n",
    "}\n",
    "\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='./',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"lor_train_v7.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure ML compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # RL framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lor_train_v7.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lor_train_v7.py\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import ray\n",
    "from gym.spaces import Discrete,Tuple, Box\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib import train\n",
    "\n",
    "class LOREnv3(MultiAgentEnv):\n",
    "    # all the actions\n",
    "    MOVEUP = 0\n",
    "    MOVEDOWN = 1\n",
    "    MOVELEFT = 2\n",
    "    MOVERIGHT = 3\n",
    "    ATTACK = 4\n",
    "    SPECIALATTACK = 5\n",
    "    GOBACK = 6\n",
    "    HOLD = 7\n",
    "    \n",
    "    action_string = {\n",
    "        MOVEUP: \"MoveUp\",\n",
    "        MOVEDOWN: \"MoveDown\",\n",
    "        MOVELEFT: \"MoveLeft\",\n",
    "        MOVERIGHT: \"MoveRight\",\n",
    "        ATTACK: \"Attack\",\n",
    "        SPECIALATTACK: \"SpecialAttack\",\n",
    "        GOBACK: \"GoBack\",\n",
    "        HOLD: \"Hold\"\n",
    "    }\n",
    "    \n",
    "    # max heath to start with\n",
    "    max_health = 1\n",
    "    \n",
    "    # space is of size n x n \n",
    "    # (0, 0) is at the top left corner\n",
    "    # x represents the vertical direction\n",
    "    # y represents the horizontal direction\n",
    "    space_size_n = 4\n",
    "    \n",
    "    # miss rate on any one attack\n",
    "    attack_miss_rate = 0\n",
    "        \n",
    "    # each attack takes some health\n",
    "    attak_power = 1\n",
    "    \n",
    "    # reward of win a game\n",
    "    game_award = 100\n",
    "    \n",
    "    invalid_action_penalty = -10\n",
    "    \n",
    "    # for special attack\n",
    "    special_attack_cool_down = 3\n",
    "    special_attack_distance = 2\n",
    "    \n",
    "    # turns to wait while dead\n",
    "    dead_hold_turns = 3\n",
    "    \n",
    "        \n",
    "    def generate_init_pos(self):\n",
    "        player1_init_pos = [0, LOREnv3.space_size_n - 1]\n",
    "        player2_init_pos = [LOREnv3.space_size_n - 1, 0]\n",
    "        \n",
    "        return player1_init_pos, player2_init_pos\n",
    "\n",
    "    \n",
    "    def is_in_opponent_base(self, player):\n",
    "        if player == self.player1  \\\n",
    "            and self.position[player][0] >= LOREnv3.space_size_n - 2 and self.position[player][1] <= 1:\n",
    "            return True\n",
    "        if player == self.player2 \\\n",
    "            and self.position[player][0] <= 1 and self.position[player][1] >= LOREnv3.space_size_n - 2:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.action_space = Discrete(len(LOREnv3.action_string))\n",
    "        \n",
    "        # the observation is a tuple: [self_pos_x, self_pos_y, self.health, pos_x, pos_y, health]\n",
    "        # start with a discrete space\n",
    "        self.observation_space = Tuple(\n",
    "            [\n",
    "                # self position in x/y\n",
    "                Box(low = 0, high = LOREnv3.space_size_n - 1, shape=(2, ), dtype=np.int16),\n",
    "                # opponent position in x/y\n",
    "                Box(low = 0, high = LOREnv3.space_size_n - 1, shape=(2, ), dtype=np.int16),\n",
    "                # self health and opponent health\n",
    "                Box(low = 0, high = LOREnv3.max_health, shape=(2, ), dtype=np.int16),\n",
    "                # self special attack cool down and opponent's cool down\n",
    "                Box(low = 0, high = LOREnv3.special_attack_cool_down, shape=(2, ), dtype=np.int16),\n",
    "                # remaining turns to revive, self and opponent. (0 means alive)\n",
    "                Box(low = 0, high = LOREnv3.dead_hold_turns, shape=(2, ), dtype=np.int16),\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.player1 = \"player1\"\n",
    "        self.player2 = \"player2\"\n",
    "        \n",
    "        self.player1_init_pos, self.player2_init_pos = self.generate_init_pos()\n",
    "        \n",
    "        #\n",
    "        self.reset()\n",
    "        \n",
    "        # For test-case inspections (compare both players' number of game wins).\n",
    "        self.player1_score = self.player2_score = 0\n",
    "\n",
    "    # reset the env\n",
    "    # return the initial observation\n",
    "    # the player1 always take action first\n",
    "    def reset(self):        \n",
    "        self.position = {\n",
    "                self.player1: self.player1_init_pos.copy(),\n",
    "                self.player2: self.player2_init_pos.copy()\n",
    "        }\n",
    "        \n",
    "        self.health = {\n",
    "            self.player1: LOREnv3.max_health,\n",
    "            self.player2: LOREnv3.max_health\n",
    "        }\n",
    "        \n",
    "        self.special_attack_cd = {\n",
    "            self.player1: 0,\n",
    "            self.player2: 0,\n",
    "        }\n",
    "        \n",
    "        self.turns_to_revive = {\n",
    "            self.player1: 0,\n",
    "            self.player2: 0\n",
    "        }\n",
    "        \n",
    "        self.last_reward = 0\n",
    "        \n",
    "        return {\n",
    "            self.player1: tuple(\n",
    "                [\n",
    "                    np.array([self.position[self.player1][0], self.position[self.player1][1]]),\n",
    "                    np.array([self.position[self.player2][0], self.position[self.player2][1]]),\n",
    "                    np.array([self.health[self.player1], self.health[self.player2]]),\n",
    "                    np.array([self.special_attack_cd[self.player1], self.special_attack_cd[self.player2]]),\n",
    "                    np.array([self.turns_to_revive[self.player1], self.turns_to_revive[self.player2]])\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def move_agent(self, player, opponent, action):\n",
    "        if self.health[player] <= 0:  # no health no action\n",
    "            return 0\n",
    "        \n",
    "        if action == LOREnv3.MOVEUP or action == LOREnv3.MOVEDOWN:\n",
    "            new_x = self.position[player][0] + (1 if action == LOREnv3.MOVEDOWN else -1)\n",
    "            if new_x < 0 or new_x >= LOREnv3.space_size_n: # invalid move\n",
    "                return self.invalid_action_penalty\n",
    "            elif (self.position[opponent][0] == new_x and self.position[opponent][1] == self.position[player][1]):\n",
    "                return self.invalid_action_penalty\n",
    "            else:\n",
    "                self.position[player][0] = new_x\n",
    "                return 0\n",
    "                \n",
    "        if action == LOREnv3.MOVELEFT or action == LOREnv3.MOVERIGHT:\n",
    "            new_y = self.position[player][1] + (1 if action == LOREnv3.MOVERIGHT else -1)\n",
    "            if new_y < 0 or new_y >= LOREnv3.space_size_n: # invalid move\n",
    "                return self.invalid_action_penalty\n",
    "            elif (self.position[opponent][1] == new_y and self.position[opponent][0] == self.position[player][0]):\n",
    "                return self.invalid_action_penalty\n",
    "            else:\n",
    "                self.position[player][1] = new_y\n",
    "                return 0\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def is_adjacent(self):\n",
    "        return (self.position[self.player1][0] == self.position[self.player2][0] and abs(self.position[self.player1][1] - self.position[self.player2][1]) <= 1) \\\n",
    "            or (self.position[self.player1][1] == self.position[self.player2][1] and abs(self.position[self.player1][0] - self.position[self.player2][0]) <= 1) \n",
    "                    \n",
    "    \n",
    "    def is_in_distance(self, distance):\n",
    "        d_square = (self.position[self.player1][0] - self.position[self.player2][0]) * (self.position[self.player1][0] - self.position[self.player2][0]) \\\n",
    "            + (self.position[self.player1][1] - self.position[self.player2][1]) * (self.position[self.player1][1] - self.position[self.player2][1])\n",
    "    \n",
    "        return d_square <= distance * distance\n",
    "    \n",
    "    \n",
    "    def take_action(self, player, opponent, action):\n",
    "        reward = 0\n",
    "        \n",
    "        if self.turns_to_revive[player] > 0: # if dead, cannot take action besides HOLD\n",
    "            if action == LOREnv3.HOLD:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = self.invalid_action_penalty\n",
    "        else:\n",
    "            if action == LOREnv3.ATTACK:\n",
    "                hit1 =  0 if not(self.is_adjacent()) or random.random() < LOREnv3.attack_miss_rate else 1\n",
    "                \n",
    "                if self.turns_to_revive[opponent] > 0: # cannot attack opponent if in revive\n",
    "                    hit1 = 0\n",
    "            \n",
    "                self.health[opponent] = self.health[opponent] - hit1  * LOREnv3.attak_power\n",
    "                reward = hit1 * LOREnv3.attak_power\n",
    "            elif action == LOREnv3.SPECIALATTACK:\n",
    "                if self.special_attack_cd[player] > 0:\n",
    "                    reward = self.invalid_action_penalty\n",
    "                elif self.is_in_distance(LOREnv3.special_attack_distance) == False: # invalid, cannot use special attack\n",
    "                    reward = self.invalid_action_penalty\n",
    "                elif self.turns_to_revive[opponent] > 0: # cannot attack opponent if in revive\n",
    "                    reward = 0\n",
    "                    # reset cd\n",
    "                    self.special_attack_cd[player] = LOREnv3.special_attack_cool_down\n",
    "                else:\n",
    "                    self.health[opponent] = self.health[opponent] - LOREnv3.attak_power\n",
    "                    reward = LOREnv3.attak_power\n",
    "                    # reset cd\n",
    "                    self.special_attack_cd[player] = LOREnv3.special_attack_cool_down\n",
    "            elif action == LOREnv3.HOLD:\n",
    "                reward = 0\n",
    "            elif action == LOREnv3.GOBACK:\n",
    "                reward = LOREnv3.max_health - self.health[player]\n",
    "            \n",
    "                # gain full health and go back to init position\n",
    "                self.health[player] = LOREnv3.max_health\n",
    "                self.position[player] = self.player1_init_pos.copy() if player == self.player1 else self.player2_init_pos.copy()\n",
    "            else: # move\n",
    "                reward = self.move_agent(player, opponent, action)\n",
    "        \n",
    "        \n",
    "        # if the opponent is dead, more reward\n",
    "        if self.health[opponent] == 0:\n",
    "            reward = reward * LOREnv3.dead_hold_turns\n",
    "        \n",
    "        # if the player reach the opponent's base     \n",
    "        if self.is_in_opponent_base(player):\n",
    "            reward = LOREnv3.game_award\n",
    "    \n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    # update state and observation based on the 2 actions\n",
    "    def step(self, action_dict):\n",
    "        # only one action each turn\n",
    "        assert len(action_dict) == 1, action_dict\n",
    "                \n",
    "        if self.player1 in action_dict:\n",
    "            player = self.player1\n",
    "            opponent = self.player2\n",
    "        else:\n",
    "            player = self.player2\n",
    "            opponent = self.player1\n",
    "        \n",
    "            \n",
    "        # update special attack CD\n",
    "        if self.special_attack_cd[player] > 0:\n",
    "            self.special_attack_cd[player] = self.special_attack_cd[player] - 1\n",
    "        \n",
    "        \n",
    "        # take action\n",
    "        reward = self.take_action(player, opponent, action_dict[player])\n",
    "        \n",
    "        # if opponent is killed\n",
    "        if self.health[opponent] == 0:\n",
    "            # opponent is send back to init position and start revive turns\n",
    "            self.health[opponent] = LOREnv3.max_health\n",
    "            self.position[opponent] = self.player1_init_pos.copy() if opponent == self.player1 else self.player2_init_pos.copy()\n",
    "            self.turns_to_revive[opponent] = LOREnv3.dead_hold_turns\n",
    "        \n",
    "        # update player's revive turns if needed\n",
    "        if self.turns_to_revive[player] > 0:\n",
    "            self.turns_to_revive[player] = self.turns_to_revive[player] - 1    \n",
    "           \n",
    "            \n",
    "        # get the new obs\n",
    "        obs = {\n",
    "            opponent: tuple(\n",
    "                [\n",
    "                    np.array([self.position[opponent][0], self.position[opponent][1]]),\n",
    "                    np.array([self.position[player][0], self.position[player][1]]),\n",
    "                    np.array([self.health[opponent], self.health[player]]),\n",
    "                    np.array([self.special_attack_cd[opponent], self.special_attack_cd[player]]),\n",
    "                    np.array([self.turns_to_revive[opponent], self.turns_to_revive[player]])\n",
    "                ]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # get the reward\n",
    "        rew = {\n",
    "            opponent: -1 * reward + self.last_reward,\n",
    "        }\n",
    "        \n",
    "        self.last_reward = reward\n",
    "        \n",
    "        done = {\n",
    "            \"__all__\": self.is_in_opponent_base(self.player1) or self.is_in_opponent_base(self.player2)\n",
    "        }\n",
    "        \n",
    "        # it is required that when done[\"__all__\"] == True, the obv/rew should include all live agent\n",
    "        if done[\"__all__\"]:\n",
    "            obs[player] = tuple(\n",
    "                [\n",
    "                    np.array([self.position[player][0], self.position[player][1]]),\n",
    "                    np.array([self.position[opponent][0], self.position[opponent][1]]),\n",
    "                    np.array([self.health[player], self.health[opponent]]),\n",
    "                    np.array([self.special_attack_cd[player], self.special_attack_cd[opponent]]),\n",
    "                    np.array([self.turns_to_revive[player], self.turns_to_revive[opponent]])\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            rew[player] = reward\n",
    "        \n",
    "\n",
    "        if self.is_in_opponent_base(self.player1) and not(self.is_in_opponent_base(self.player2)):\n",
    "            self.player1_score += 1\n",
    "        elif self.is_in_opponent_base(self.player2) and not(self.is_in_opponent_base(self.player1)):\n",
    "            self.player2_score += 1\n",
    "\n",
    "        return obs, rew, done, {}\n",
    "\n",
    "\n",
    "class LORHeuristic(Policy):\n",
    "    \"\"\"\n",
    "    Heuristic policy\n",
    "    Random pick between one of the following.\n",
    "    \n",
    "    cautious\n",
    "    \n",
    "    if self.health > 1:\n",
    "        if self and opponent is adjacent:\n",
    "            attack\n",
    "        elif can use special attack and within range:\n",
    "            special attack\n",
    "        else:\n",
    "            move torwards the opponent\n",
    "    else:\n",
    "        if self and opponent is adjacent:\n",
    "            move away from the opponent \n",
    "        elif can use special attack and within range:\n",
    "            use special attack\n",
    "        else:\n",
    "            attack\n",
    "            \n",
    "    reckless\n",
    "    if can move towards opponent base\n",
    "        move towards it\n",
    "    elif self and opponent is adjacent:\n",
    "        attack\n",
    "    elif can use special attack and within range:\n",
    "        use special attack\n",
    "    else\n",
    "        move torwards the opponent\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.exploration = self._create_exploration()\n",
    "        self.space_size = LOREnv3.space_size_n\n",
    "    \n",
    "    def can_use_special_attack(self, obv, cd):\n",
    "        d_square = (obv[0] - obv[2])*(obv[0] - obv[2]) + (obv[1] - obv[3]) * (obv[1] - obv[3])\n",
    "        return d_square <= LOREnv3.special_attack_distance * LOREnv3.special_attack_distance and \\\n",
    "            cd <= 0\n",
    "    \n",
    "    def can_move_torwards_opponent_base(self, obv):\n",
    "        # assume that the policy always play player2, and target (0, size -1)\n",
    "        self_x = obv[0]\n",
    "        self_y = obv[1]\n",
    "        op_x = obv[2]\n",
    "        op_y = obv[3]\n",
    "        \n",
    "        actions = []\n",
    "        \n",
    "        if self_x > 0 and not(op_y == self_y and op_x == self_x - 1): # can reduce x\n",
    "            actions.append(LOREnv3.MOVEUP)\n",
    "        \n",
    "        if self_y < LOREnv3.space_size_n -1 and not(op_y == self_y + 1 and op_x == self_x): # can increase y\n",
    "            actions.append(LOREnv3.MOVERIGHT)\n",
    "            \n",
    "        action = -1\n",
    "        if len(actions) > 0:\n",
    "            action = actions[random.randrange(len(actions))]\n",
    "        \n",
    "        return action\n",
    "                \n",
    "            \n",
    "    \n",
    "    def take_reckless_action(self, obv):\n",
    "        # each ob is np array (self.x, self.y, oponent.x, oppoennt.y, self.health, opponent.health)\n",
    "        self_x = obv[0]\n",
    "        self_y = obv[1]\n",
    "        op_x = obv[2]\n",
    "        op_y = obv[3]\n",
    "        self_h = obv[4]\n",
    "        op_h = obv[5]\n",
    "        self_cd = obv[6]\n",
    "        op_cd = obv[7]\n",
    "        self_revive_cd = obv[8]\n",
    "        op_revive_cd = obv[9]\n",
    "        \n",
    "        if self_revive_cd > 0:\n",
    "            return LOREnv3.HOLD\n",
    "             \n",
    "        \n",
    "        try_move_towards_base = self.can_move_torwards_opponent_base(obv)\n",
    "        \n",
    "        \n",
    "        if (self_x == op_x and abs(self_y - op_y) <= 1) or (self_y == op_y and abs(self_x - op_x) <= 1):\n",
    "            return LOREnv3.ATTACK\n",
    "        elif self.can_use_special_attack(obv, self_cd):\n",
    "            return LOREnv3.SPECIALATTACK\n",
    "        elif try_move_towards_base  >= 0:\n",
    "            return try_move_towards_base \n",
    "        else: # randomly move left or down or hold\n",
    "            actions = [LOREnv3.HOLD]\n",
    "        \n",
    "            if self_y > 0 and not(op_y == self_y - 1 and op_x == self_x): \n",
    "                actions.append(LOREnv3.MOVELEFT)\n",
    "            \n",
    "            if self_x < self.space_size - 1 and not(op_y == self_y and op_x == self_x + 1): \n",
    "                actions.append(LOREnv3.MOVEDOWN)\n",
    "            \n",
    "            return actions[random.randrange(len(actions))]\n",
    "                \n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "\n",
    "        return [self.take_reckless_action(x) for x in obs_batch], [], {}\n",
    "    \n",
    "        #return [self.take_reckless_action(x) if random.random() < 0 else self.take_cautious_action(x)  for x in obs_batch], [], {}\n",
    "    \n",
    "    def learn_on_batch(self, samples):\n",
    "        pass\n",
    "\n",
    "    def get_weights(self):\n",
    "        pass\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "    \n",
    "def on_train_result(info):\n",
    "    '''Callback on train result to record metrics returned by trainer.\n",
    "    '''\n",
    "    run = Run.get_context()\n",
    "    run.log(\n",
    "        name='episode_reward_mean',\n",
    "        value=info[\"result\"][\"episode_reward_mean\"])\n",
    "    run.log(\n",
    "        name='episodes_total',\n",
    "        value=info[\"result\"][\"episodes_total\"])\n",
    "\n",
    "import argparse\n",
    "\n",
    "DEFAULT_RAY_ADDRESS = 'localhost:6379'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--iterations\", type=int)\n",
    "    parser.add_argument(\"--numworkers\", type=int)\n",
    "    parser.add_argument(\"--basepolicy\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "            \n",
    "    base_policy_cls=globals()[args.basepolicy]\n",
    "    \n",
    "    def select_policy(agent_id):\n",
    "        if agent_id == \"player1\":\n",
    "            return \"learned\"\n",
    "        else:\n",
    "            return args.basepolicy\n",
    "    \n",
    "    ray.init(address=DEFAULT_RAY_ADDRESS)\n",
    "    \n",
    "    env = LOREnv3({})\n",
    "    \n",
    "    config = {\n",
    "        \"env\": LOREnv3,\n",
    "        \"gamma\": 0.9,\n",
    "        \"num_workers\": args.numworkers,\n",
    "        \"num_envs_per_worker\": 4,\n",
    "        \"rollout_fragment_length\": 10,\n",
    "        \"train_batch_size\": 1000,\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": [\"learned\"],\n",
    "            \"policies\": {\n",
    "                args.basepolicy: (base_policy_cls, env.observation_space, env.action_space, {}),\n",
    "                \"learned\": (None, env.observation_space, env.action_space, {\n",
    "                    \"model\": {\n",
    "                            \"use_lstm\": True\n",
    "                    },\n",
    "                }),\n",
    "            },\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"callbacks\": {\"on_train_result\": on_train_result},\n",
    "    }\n",
    "\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    \n",
    "    trainer_obj = DQNTrainer(config=config)\n",
    "    env = trainer_obj.workers.local_worker().env\n",
    "    for _ in range(args.iterations):\n",
    "        results = trainer_obj.train()\n",
    "        \n",
    "        run.log(name='player1_score',\n",
    "            value=sum(trainer_obj.workers.foreach_worker(lambda w : w.env.player1_score)))\n",
    "        \n",
    "        run.log(name='player2_score',\n",
    "            value=sum(trainer_obj.workers.foreach_worker(lambda w : w.env.player2_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name='rllib-lor-v7'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "run = exp.submit(config=rl_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec008ce5e0d84c529c940980927ee886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_RLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sdk_v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/rllib-lor-v7/runs/rllib-lor-v7_1593063544_4192062b?wsid=/subscriptions/1aefdc5e-3a7c-4d71-a9f9-f5d3b03be19a/resourcegroups/EDATRG/workspaces/fepEDATest\", \"run_id\": \"rllib-lor-v7_1593063544_4192062b\", \"run_properties\": {\"run_id\": \"rllib-lor-v7_1593063544_4192062b\", \"created_utc\": \"2020-06-25T05:39:58.743171Z\", \"properties\": {}, \"tags\": {\"cluster_coordination_timeout_seconds\": \"3600\"}, \"end_time_utc\": \"2020-06-25T05:51:53.677746Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/reinforcementlearning.txt\": \"https://fepedatest5565713082.blob.core.windows.net/azureml/ExperimentRun/dcid.rllib-lor-v7_1593063544_4192062b/azureml-logs/reinforcementlearning.txt?sv=2019-02-02&sr=b&sig=UWetHbYUrYFScmBhpuoT3eENaoja4bADWqewr1uxirc%3D&st=2020-06-25T05%3A41%3A59Z&se=2020-06-25T13%3A51%3A59Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/reinforcementlearning.txt\"]], \"run_duration\": \"0:11:54\", \"cluster_coordination_timeout_seconds\": \"3600\"}, \"child_runs\": [{\"run_id\": \"rllib-lor-v7_1593063544_4192062b_worker\", \"run_number\": 3, \"metric\": null, \"status\": \"CancelRequested\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-06-25T05:45:35.629882Z\", \"end_time\": \"\", \"created_time\": \"2020-06-25T05:40:15.18312Z\", \"created_time_dt\": \"2020-06-25T05:40:15.18312Z\", \"duration\": \"0:11:44\"}, {\"run_id\": \"rllib-lor-v7_1593063544_4192062b_head\", \"run_number\": 2, \"metric\": null, \"status\": \"Completed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-06-25T05:44:30.984662Z\", \"end_time\": \"2020-06-25T05:51:47.558192Z\", \"created_time\": \"2020-06-25T05:40:14.436713Z\", \"created_time_dt\": \"2020-06-25T05:40:14.436713Z\", \"duration\": \"0:11:33\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-06-25T05:39:58.9579654Z][Info]Starting reinforcement learning run with id rllib-lor-v7_1593063544_4192062b.\\n[2020-06-25T05:40:04.6878419Z][Info]Starting head node child run with id rllib-lor-v7_1593063544_4192062b_head.\\n[2020-06-25T05:40:14.5878505Z][Info]Starting worker child run with id rllib-lor-v7_1593063544_4192062b_worker.\\n[2020-06-25T05:51:56.3325386Z][Info]Some child runs have reached terminal state. All active child runs will be cancelled. The run Ids that reached terminal state are: rllib-lor-v7_1593063544_4192062b_head.\\n[2020-06-25T05:51:56.3730495Z][Info]Updating status of child run with Id rllib-lor-v7_1593063544_4192062b_worker from Running to Completed, since one of the child runs has reached a terminal state.\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.6.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "#run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    \"--iterations\": 2000,\n",
    "    \"--numworkers\": 6,\n",
    "    \"--basepolicy\": \"LORHeuristic\"\n",
    "}\n",
    "\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='./',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"lor_train_v7.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure ML compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # RL framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=7200,\n",
    ")\n",
    "\n",
    "experiment_name='rllib-lor-v7'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "run = exp.submit(config=rl_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94997b3225094761b1c2b9d86860f299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_RLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sdk_v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/rllib-lor-v7/runs/rllib-lor-v7_1593064970_e661fa6f?wsid=/subscriptions/1aefdc5e-3a7c-4d71-a9f9-f5d3b03be19a/resourcegroups/EDATRG/workspaces/fepEDATest\", \"run_id\": \"rllib-lor-v7_1593064970_e661fa6f\", \"run_properties\": {\"run_id\": \"rllib-lor-v7_1593064970_e661fa6f\", \"created_utc\": \"2020-06-25T06:02:52.369659Z\", \"properties\": {}, \"tags\": {\"cluster_coordination_timeout_seconds\": \"3600\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/reinforcementlearning.txt\": \"https://fepedatest5565713082.blob.core.windows.net/azureml/ExperimentRun/dcid.rllib-lor-v7_1593064970_e661fa6f/azureml-logs/reinforcementlearning.txt?sv=2019-02-02&sr=b&sig=Q%2B3RtamjunMxI4tyxswtjSQY13FToU5A7Mj2OXQpSRI%3D&st=2020-06-25T06%3A15%3A10Z&se=2020-06-25T14%3A25%3A10Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/reinforcementlearning.txt\"]], \"run_duration\": \"0:22:17\", \"cluster_coordination_timeout_seconds\": \"3600\"}, \"child_runs\": [{\"run_id\": \"rllib-lor-v7_1593064970_e661fa6f_worker\", \"run_number\": 6, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-06-25T06:11:35.179476Z\", \"end_time\": \"\", \"created_time\": \"2020-06-25T06:02:59.31426Z\", \"created_time_dt\": \"2020-06-25T06:02:59.31426Z\", \"duration\": \"0:22:11\"}, {\"run_id\": \"rllib-lor-v7_1593064970_e661fa6f_head\", \"run_number\": 5, \"metric\": null, \"status\": \"Running\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-06-25T06:05:37.996042Z\", \"end_time\": \"\", \"created_time\": \"2020-06-25T06:02:58.80635Z\", \"created_time_dt\": \"2020-06-25T06:02:58.80635Z\", \"duration\": \"0:22:11\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-06-25T06:02:53.7419970Z][Info]Starting reinforcement learning run with id rllib-lor-v7_1593064970_e661fa6f.\\n[2020-06-25T06:02:58.3447917Z][Info]Starting head node child run with id rllib-lor-v7_1593064970_e661fa6f_head.\\n[2020-06-25T06:02:58.8757228Z][Info]Starting worker child run with id rllib-lor-v7_1593064970_e661fa6f_worker.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.6.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "#run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
